{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Object Detection Models in SageMaker with Augmented Manifests\n",
    "\n",
    "This notebook demonstrates the use of an \"augmented manifest\" to train an object detection machine learning model with AWS SageMaker.\n",
    "\n",
    "**Note:** This notebook was adapted from: https://github.com/awslabs/amazon-sagemaker-examples.git for the Belong Deeplens Innovation Sprint. This lab is based on a small dataset consisting of a pre-labelled set of images containing 500 images of analogue guages. The labelling was specifically tasked to box the analogue guage. \n",
    "\n",
    "This detailed lab guide can be found at:\n",
    "https://aws-computer-vision.jacobcantwell.com/\n",
    "\n",
    "This dataset and the attached augmented manifest file can be found at:\n",
    "https://aws-computer-vision.jacobcantwell.com/jupyter/analogue-guage-detection.zip\n",
    "\n",
    "Author: Jacob Cantwell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise project variables\n",
    "\n",
    "Below we initialise the location of files and objects that we need to set up the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "## Updated below to your local lab team S3 bucket\n",
    "bucket_name = \"deeplens-[YOUR TEAM NAME]-belong-lab\" # Replace '[YOUR TEAM NAME]' with your lab teams bucket name.\n",
    "\n",
    "# Create unique job name \n",
    "job_name_prefix = '[YOUR TEAM NAME]' # Enter your lab team name or other unique identifyer.\n",
    "\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "job_name = job_name_prefix + timestamp\n",
    "\n",
    "## URL of the training images and Augmented Manifest file being used for this lab\n",
    "# Replace if needed.\n",
    "training_data_url = \"https://aws-computer-vision.jacobcantwell.com/jupyter/analogue-guage-detection.zip\"\n",
    "\n",
    "## Image dataset directory\n",
    "# Is used for both local and S3 image directory so is hard dependency on local directory name for images.\n",
    "image_dataset_prefix = 'training-images'\n",
    "\n",
    "manifest_prefix = 'manifests' # S3 folder to store and process the manifest file. \n",
    "train_manifest = 'train.manifest'\n",
    "validate_manifest = 'validate.manifest'\n",
    "\n",
    "## Format full path for training and validation manifests\n",
    "s3_train_manifest = 's3://{}/{}/{}'.format(bucket_name, manifest_prefix, train_manifest)\n",
    "s3_validate_manifest = 's3://{}/{}/{}'.format(bucket_name, manifest_prefix, validate_manifest)\n",
    "\n",
    "## Output folder for training data and model/\n",
    "s3_output_path = 's3://{}/training-output'.format(bucket_name)\n",
    "\n",
    "## Print the paths just to validate\n",
    "print(\"Training Job Name: {}\".format(job_name))\n",
    "print(\"S3 Bucket: {}\".format(bucket_name))\n",
    "print(\"Augmented manifest for training data: {}\".format(s3_train_manifest))\n",
    "print(\"Augmented manifest for validation data: {}\".format(s3_validate_manifest))\n",
    "print(\"Output training data path: {}\".format(s3_output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Here we define the training image containing the semantic segmentation algorithm, and instantiate a SageMaker session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import json\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "training_image = sagemaker.amazon.amazon_estimator.get_image_uri(boto3.Session().region_name, 'object-detection', repo_version='latest')\n",
    "\n",
    "print ('Execution Role: {}'.format(role))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Augmented Manifest Format\n",
    "\n",
    "Augmented manifests provide two key benefits. First, the format is consistent with that of a labelling job output manifest. This means that you can take your output manifests from a Ground Truth labelling job and, whether the dataset objects were entirely human-labelled, entirely machine-labelled, or anything in between, and use them as inputs to SageMaker training jobs - all without any additional translation or reformatting! Second, the dataset objects and their corresponding ground truth labels/annotations are captured *inline*. This effectively reduces the required number of channels by half, since you no longer need one channel for the dataset objects alone and another for the associated ground truth labels/annotations.\n",
    "\n",
    "The augmented manifest format is essentially the [json-lines format](http://jsonlines.org/), also called the new-line delimited JSON format. This format consists of an arbitrary number of well-formed, fully-defined JSON objects, each on a separate line. Augmented manifests must contain a field that defines a dataset object, and a field that defines the corresponding annotation. Let's look at an example for an object detection problem.\n",
    "\n",
    "\n",
    "The Ground Truth output format is discussed more fully for various types of labelling jobs in the [official documenation](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-data-output.html).\n",
    "\n",
    "{<span style=\"color:blue\">\"source-ref\"</span>: \"s3://bucket_name/path_to_a_dataset_object.jpeg\", <span style=\"color:blue\">\"labeling-job-name\"</span>: {\"annotations\":[{\"class_id\":\"0\",`<bounding box dimensions>`}],\"image_size\":[{`<image size simensions>`}]}\n",
    "\n",
    "The first field will always be either `source` our `source-ref`. This defines an individual dataset object. The name of the second field depends on whether the labelling job was created from the SageMaker console or through the Ground Truth API. If the job was created through the console, then the name of the field will be the labelling job name. Alternatively, if the job was created through the API, then this field maps to the `LabelAttributeName` parameter in the API. \n",
    "\n",
    "The training job request requires a parameter called `AttributeNames`. This should be a two-element list of strings, where the first string is \"source-ref\", and the second string is the label attribute name from the augmented manifest. This corresponds to the <span style=\"color:blue\">blue text</span> in the example above. In this case, we would define `attribute_names = [\"source-ref\", \"labeling-job-name\"]`.\n",
    "\n",
    "*Be sure to carefully inspect your augmented manifest so that you can define the `attribute_names` variable below.*\n",
    "\n",
    "The key feature of the augmented manifest is that it has both the data object itself (i.e., the image), and the annotation in-line in a single JSON object. Note that the `annotations` keyword contains dimensions and coordinates (e.g., width, top, height, left) for bounding boxes! The augmented manifest can contain an arbitrary number of lines, as long as each line adheres to this format.\n",
    "\n",
    "Let's discuss this format in more detail by describing each parameter of this JSON object format.\n",
    "\n",
    "* The `source-ref` field defines a single dataset object, which in this case is an image over which bounding boxes should be drawn. Note that the name of this field is arbitrary. \n",
    "* The `object-detection-job-name` field defines the ground truth bounding box annotations that pertain to the image identified in the `source-ref` field. As mentioned above, note that the name of this field is arbitrary. You must take care to define this field in the `AttributeNames` parameter of the training job request, as shown later on in this notebook.\n",
    "* Because this example augmented manifest was generated through a Ground Truth labelling job, this example also shows an additional field called `object-detection-job-name-metadata`. This field contains various pieces of metadata from the labelling job that produced the bounding box annotation(s) for the associated image, e.g., the creation date, confidence scores for the annotations, etc. This field is ignored during the training job. However, to make it as easy as possible to translate Ground Truth labelling jobs into trained SageMaker models, it is safe to include this field in the augmented manifest you supply to the training job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Augmented Manifest and Training Images\n",
    "\n",
    "Download the training images and manifest file to this Notebook's workspace.\n",
    "**Note:** For larger datasets its more efficient to download direct to S3 instead of to the Notebook!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from zipfile import ZipFile\n",
    "\n",
    "temp_zipfile = './analogue-guage-detection.zip'\n",
    "## Download the analogue guage training manifest and image dataset as a ZIP attached to this lab.\n",
    "urllib.request.urlretrieve(training_data_url, temp_zipfile)\n",
    "\n",
    "## Unzip the image set\n",
    "with ZipFile(temp_zipfile, 'r') as zipObj:\n",
    "   # Extract all the contents of zip file in current directory\n",
    "   zipObj.extractall()\n",
    "\n",
    "## Delete the data .ZIP file top conserve space \n",
    "os.remove(temp_zipfile)\n",
    "\n",
    "## Print out the file sin local workspace as validation\n",
    "print('Files in local workspace:')\n",
    "os.listdir()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate the Manifest into Training and Validation Files.\n",
    "\n",
    "While building a model, Sagemaker needs some images to be excluded from the training process so that they can be used to validate the accuracy of the model being developed. If the model validates on images it has seen before in training the results will be artificially high.\n",
    "\n",
    "To allow for this we separate out a small subset (in this case 10%) of the training images from the augmented manifest file and add to the validation manifest. The remaining of the labelled imaged listed in the augmented manifest are saved into the training manifest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Create the training and validation manifests from the labelled Augmented Manifest\n",
    "print ('Creating the training and validation manifests from the labelled augmented manifest:')\n",
    "\n",
    "local_manifest = 'augmented-manifest.json'\n",
    "validation_ratio = 0.1      # Ratio of images to separate to validation manifest\n",
    "\n",
    "##############################################################\n",
    "# Get all manifest source-ref lines into sourceref_array\n",
    "sourceref_array = []\n",
    "print('Reading Augmented Manifest file to sourceref_array:')\n",
    "with open(local_manifest) as manifest_file:\n",
    "    for line in manifest_file:\n",
    "        # Localref replaces a placeholder S3 bucket in the manifest with the local value.\n",
    "        localref = line.replace( '[BUCKET_AND_PATH]', '{}/{}'.format(bucket_name, image_dataset_prefix))\n",
    "        sourceref_array.append(localref)\n",
    "\n",
    "dataset_size = len(sourceref_array)\n",
    "print ('Found: {} image source-refs in augmented manifest'.format(dataset_size))\n",
    "print ('complete.')\n",
    "\n",
    "##############################################################\n",
    "# Calculate training and validation image manifest lengths.\n",
    "print ('\\nCalculate training and validation manifest lengths at {} of complete image dataset.'.format(validation_ratio))\n",
    "validation_size = int(round(dataset_size * float(validation_ratio)))\n",
    "training_size = dataset_size - validation_size\n",
    "\n",
    "print ('Total Dataset Images: {}'.format(dataset_size))\n",
    "print ('Training Images, {}'.format(training_size))\n",
    "print ('Validation Images: {}'.format(validation_size))\n",
    "print ('complete.')\n",
    "\n",
    "##############################################################\n",
    "# Get random image references for validation manifest and write to workspace\n",
    "print ('Nominate {} random image references for validation manifest'.format(validation_size))\n",
    "validation_array = []\n",
    "\n",
    "for i in range(validation_size):\n",
    "    # get current size of sourceref_array as items are pop'ed.\n",
    "    dataset_remain_size = len(sourceref_array)\n",
    "    # Calculate a random int between 0 and current size of sourceref_array\n",
    "    rand_val = random.randrange(0, dataset_remain_size);\n",
    "    # Pop the random key value off sourceref_array and into the validation_array\n",
    "    validation_array.append(sourceref_array.pop(rand_val))\n",
    "\n",
    "print ('{} image refs applied to validation manifest'.format(len(validation_array)))\n",
    "print ('complete.')\n",
    "\n",
    "##############################################################\n",
    "# Write training and validation manifests to workspace\n",
    "print ('\\nWrite training and validation manifests to workspace:')\n",
    "\n",
    "# Write sourceref_array lines not split out to validation_array to training manifest file\n",
    "print ('Write Training manifest to: {}'.format(train_manifest))\n",
    "with open(train_manifest, 'w') as f:\n",
    "    for line in sourceref_array:\n",
    "        f.write(line)\n",
    "\n",
    "print ('complete.')\n",
    "  \n",
    "# Write validation_array lines to validation manifest file\n",
    "print ('Write Validation manifest to: {}'.format(validate_manifest))\n",
    "with open(validate_manifest, 'w') as f:\n",
    "    for line in validation_array:\n",
    "        f.write(line)\n",
    "print ('complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the Training Image Dataset and Manifest Files to S3 for Sagemaker.\n",
    "\n",
    "In proceeding steps we are going to initiate a dedicated instance to build and train the object detection model. Because this instance doesn't have access to the dataset and manifest files that were processed in this notebook, we need to upload these to S3 so the training instance can access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the training manifest to S3\n",
    "print ('Uploading {} to {}'.format(train_manifest, s3_train_manifest))\n",
    "s3.meta.client.upload_file(train_manifest, bucket_name, '{}/{}'.format(manifest_prefix, train_manifest))\n",
    "print ('complete\\n')\n",
    "\n",
    "# Upload the validation manifest to S3\n",
    "print ('Uploading {} to {}'.format(validate_manifest, s3_validate_manifest))\n",
    "s3.meta.client.upload_file(validate_manifest, bucket_name, '{}/{}'.format(manifest_prefix, validate_manifest))\n",
    "print ('complete\\n')\n",
    "\n",
    "# Upload the training image dataset\n",
    "print ('Uploading training images in {} to {}'.format(image_dataset_prefix, bucket_name))\n",
    "for filename in os.listdir(image_dataset_prefix):\n",
    "    image_path = '{}/{}'.format(image_dataset_prefix, filename)\n",
    "    s3.meta.client.upload_file(image_path, bucket_name, image_path)\n",
    "    print ('Successfully uploaded Image: {}'.format(image_path))\n",
    "\n",
    "print ('Upload Complete\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview Input Data\n",
    "\n",
    "Let's read the augmented manifest so we can inspect its contents to better understand the format and to verify its now accessible from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_manifest_s3_key = s3_train_manifest.split(bucket_name)[1][1:]\n",
    "s3_obj = s3.Object(bucket_name, augmented_manifest_s3_key)\n",
    "augmented_manifest = s3_obj.get()['Body'].read().decode('utf-8')\n",
    "augmented_manifest_lines = augmented_manifest.split('\\n')\n",
    "\n",
    "num_training_samples = len(augmented_manifest_lines) # Compute number of training samples for use in training job request.\n",
    "\n",
    "\n",
    "print('Preview of Augmented Manifest File Contents')\n",
    "print('-------------------------------------------')\n",
    "print('\\n')\n",
    "\n",
    "for i in range(2):\n",
    "    print('Line {}'.format(i+1))\n",
    "    print(augmented_manifest_lines[i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the attribute names:\n",
    "In the previous step you can see the name of the object contain all the labelling data is \"vehicle-class\" and so the attribute names for this manifest is source-ref and vehicle-class. This custom attribute was configured during the labelling task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using a different manifest file than the one given then make sure to update this field accordingly.\n",
    "\n",
    "attribute_names = [\"source-ref\",\"vehicle-class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Training Parameters:\n",
    "In this step we construct the parameters for the training job.\n",
    "\n",
    "+ The required parameters have been derived or entered in the steps above.\n",
    "+ The hyperparameters are beyond the scope of this lab and have been set to sane defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_params = \\\n",
    "{\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": training_image,\n",
    "        \"TrainingInputMode\": \"Pipe\"\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": s3_output_path\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 1,   \n",
    "        \"InstanceType\": \"ml.p3.2xlarge\",\n",
    "        \"VolumeSizeInGB\": 50\n",
    "    },\n",
    "    \"TrainingJobName\": job_name,\n",
    "    \"HyperParameters\": { \n",
    "         \"base_network\": \"resnet-50\",\n",
    "         \"use_pretrained_model\": \"1\",\n",
    "         \"num_classes\": \"4\",\n",
    "         \"mini_batch_size\": \"1\",\n",
    "         \"epochs\": \"50\",\n",
    "         \"learning_rate\": \"0.001\",\n",
    "         \"lr_scheduler_step\": \"3,6\",\n",
    "         \"lr_scheduler_factor\": \"0.1\",\n",
    "         \"optimizer\": \"rmsprop\",\n",
    "         \"momentum\": \"0.9\",\n",
    "         \"weight_decay\": \"0.0005\",\n",
    "         \"overlap_threshold\": \"0.5\",\n",
    "         \"nms_threshold\": \"0.45\",\n",
    "         \"image_shape\": \"300\",\n",
    "         \"label_width\": \"350\",\n",
    "         \"num_training_samples\": str(num_training_samples)\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 86400\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"AugmentedManifestFile\",\n",
    "                    \"S3Uri\": s3_train_manifest,\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                    \"AttributeNames\": attribute_names\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"application/x-recordio\",\n",
    "            \"RecordWrapperType\": \"RecordIO\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"AugmentedManifestFile\",\n",
    "                    \"S3Uri\": s3_validate_manifest,\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                    \"AttributeNames\": attribute_names\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"application/x-recordio\",\n",
    "            \"RecordWrapperType\": \"RecordIO\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    " \n",
    "print('Training job name: {}'.format(job_name))\n",
    "print('\\nInput Data Location: {}'.format(training_params['InputDataConfig'][0]['DataSource']['S3DataSource']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start The Training Job  \n",
    "\n",
    "Now we create the Amazon SageMaker training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client(service_name='sagemaker')\n",
    "client.create_training_job(**training_params)\n",
    "\n",
    "# Confirm that the training job has started\n",
    "status = client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "print('Training job current status: {}'.format(status))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitor Progress of The Training Job  \n",
    "\n",
    "Execute the below cell to get 30 second status updates from the training job.\n",
    "\n",
    "**Note:** Its expected that the training will take about 15 minutes. Is a good time to take a short break while the instance spins up and the training returns some meaningful data.\n",
    "\n",
    "You can also view the progress and results of the training in the Sagemaker console.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TrainingJobStatus = client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "SecondaryStatus = client.describe_training_job(TrainingJobName=job_name)['SecondaryStatus']\n",
    "print(TrainingJobStatus, SecondaryStatus)\n",
    "while TrainingJobStatus !='Completed' and TrainingJobStatus!='Failed':\n",
    "    time.sleep(30)\n",
    "    TrainingJobStatus = client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "    SecondaryStatus = client.describe_training_job(TrainingJobName=job_name)['SecondaryStatus']\n",
    "    print(TrainingJobStatus, SecondaryStatus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "That's it! Let's review what we've learned. \n",
    "* Augmented manifests are a new format that provide a seamless interface between Ground Truth labelling jobs and SageMaker training jobs. \n",
    "* In augmented manifests, you specify the dataset objects and the associated annotations in-line.\n",
    "* Be sure to pay close attention to the `AttributeNames` parameter in the training job request. The strings you specify in this field must correspond to those that are present in your augmented manifest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}